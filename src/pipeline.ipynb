{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SZeifQf5b7C"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVvxgmTq5dl0"
   },
   "outputs": [],
   "source": [
    "# !cp drive/My\\ Drive/11785/hw4_p2/src/dataloader.py .\n",
    "# !cp drive/My\\ Drive/11785/hw4_p2/src/models.py .\n",
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rpm6gTcb5Y7Y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import Levenshtein\n",
    "import os\n",
    "import importlib\n",
    "import dataloader\n",
    "import models\n",
    "\n",
    "mode = \"actual\"\n",
    "colab = False\n",
    "cuda = torch.cuda.is_available()\n",
    "num_workers = 4 if cuda else 0 \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "def reload():\n",
    "    importlib.reload(dataloader)\n",
    "    importlib.reload(models)\n",
    "    \n",
    "print(\"mode: %s\" % mode)\n",
    "print(\"torch version: %s\" % torch.__version__)\n",
    "print(\"np version: %s\" % np.__version__)\n",
    "print(\"cuda: %s\" % cuda)\n",
    "print(\"num_workers: %s\" % num_workers)\n",
    "print(\"device: %s\" % device)\n",
    "print(\"colab: %s\" % colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63Im9WI_5Y7d"
   },
   "outputs": [],
   "source": [
    "def generate_mask(lens, batch_size):\n",
    "    lens = torch.tensor(lens)\n",
    "    max_len = torch.max(lens)\n",
    "    mask = (torch.arange(0, max_len).repeat(batch_size, 1) < \\\n",
    "                lens.unsqueeze(1).expand(batch_size, max_len)).int()\n",
    "    return mask\n",
    "\n",
    "def get_avg_edit_distances(preds, golds):\n",
    "    sum_edit_dists = 0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        # calculate Levenshtein distance as accuracy\n",
    "        edit_dist = Levenshtein.distance(pred, gold)\n",
    "        sum_edit_dists +=edit_dist\n",
    "    return sum_edit_dists / len(preds)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_perplexity = 0.0\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1) Iterate through your loader\n",
    "    for batch_idx, sample in enumerate(tqdm(train_loader)):\n",
    "        if (batch_idx == 0):\n",
    "            print(\"output: \", dataloader.transform_index_to_letter(sample.outputs.cpu().numpy(), index2letter, [letter2index['<pad>']])[0])\n",
    "            print(\"target: \", dataloader.transform_index_to_letter(sample.targets.cpu().numpy(), index2letter, [letter2index['<pad>']])[0])\n",
    "        assert sample.inputs.shape[1] == max(sample.inputs_lens)\n",
    "        assert sample.outputs.shape[1] == max(sample.outputs_lens)\n",
    "        assert sample.targets.shape[1] == max(sample.targets_lens)\n",
    "\n",
    "        inputs, inputs_lens = sample.inputs, sample.inputs_lens\n",
    "        outputs, outputs_lens = sample.outputs, sample.outputs_lens\n",
    "        targets, targets_lens = sample.targets, sample.targets_lens\n",
    "\n",
    "        # 2) TODO: Use torch.autograd.set_detect_anomaly(True) to get notices about gradient explosion\n",
    "\n",
    "        # 3) Set the inputs to the device.\n",
    "        inputs, outputs, targets = inputs.to(device), outputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4) Pass your inputs, and length of speech into the model.\n",
    "        predictions = model(None, None, text=outputs, isTrain=True)\n",
    "\n",
    "        # 5) Generate a mask based on the lengths of the text to create a masked loss. \n",
    "        # 5.1) Ensure the mask is on the device and is the correct shape.\n",
    "        # 6) If necessary, reshape your predictions and origianl text input \n",
    "        # 6.1) Use .contiguous() if you need to. \n",
    "        targets_mask = generate_mask(targets_lens, inputs.shape[0]).to(device)\n",
    "\n",
    "        # 7) Use the criterion to get the loss.\n",
    "        all_loss = criterion(predictions.view(-1, predictions.size(2)), targets.view(-1))\n",
    "\n",
    "        # 8) Use the mask to calculate a masked loss.\n",
    "        loss = torch.sum(all_loss * targets_mask.view(-1)) / torch.sum(targets_mask)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_perplexity += torch.exp(loss).item()\n",
    "\n",
    "        # 9) Run the backward pass on the masked loss. \n",
    "        loss.backward()\n",
    "\n",
    "        # TODO: 10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
    "\n",
    "        # 11) Take a step with your optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    # 12) Normalize the masked loss\n",
    "    running_loss /= len(train_loader)\n",
    "    running_perplexity /= len(train_loader)\n",
    "\n",
    "    # 13) Optionally print the training loss after every N batches\n",
    "    print('Training Loss: ', running_loss, 'Training Perplexity: ', running_perplexity, \n",
    "          'Time: ',end_time - start_time, 's')\n",
    "        \n",
    "    return running_loss, running_perplexity\n",
    "\n",
    "def evaluate_epoch(model, eval_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_perplexity = 0.0\n",
    "        running_edit_distance = 0.0\n",
    "\n",
    "        for batch_idx, sample in enumerate(tqdm(eval_loader)):\n",
    "            assert sample.inputs.shape[1] == max(sample.inputs_lens)\n",
    "            assert sample.outputs.shape[1] == max(sample.outputs_lens)\n",
    "            assert sample.targets.shape[1] == max(sample.targets_lens)\n",
    "\n",
    "            inputs, inputs_lens = sample.inputs, sample.inputs_lens\n",
    "            outputs, outputs_lens = sample.outputs, sample.outputs_lens\n",
    "            targets, targets_lens = sample.targets, sample.targets_lens\n",
    "\n",
    "            inputs, outputs, targets = inputs.to(device), outputs.to(device), targets.to(device)\n",
    "\n",
    "            # training mode\n",
    "            predictions = model(None, None, text=outputs, isTrain=True)\n",
    "            targets_mask = generate_mask(targets_lens, inputs.shape[0]).to(device)\n",
    "\n",
    "            all_loss = criterion(predictions.view(-1, predictions.size(2)), targets.view(-1))\n",
    "            loss = torch.sum(all_loss * targets_mask.view(-1)) / torch.sum(targets_mask)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_perplexity += torch.exp(loss).item()\n",
    "            \n",
    "            # generate mode\n",
    "            predictions = model(None, None, text=outputs, isTrain=False, batch_size=inputs.shape[0])\n",
    "            \n",
    "            # TODO: use random search/greedy search here\n",
    "            predictions_texts = \\\n",
    "                dataloader.transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy(), index2letter, [letter2index['<eos>'], letter2index['<pad>']])            \n",
    "            targets_texts = \\\n",
    "                dataloader.transform_index_to_letter(targets.detach().cpu().numpy(), index2letter, [letter2index['<eos>'], letter2index['<pad>']])\n",
    "        \n",
    "            running_edit_distance += get_avg_edit_distances(predictions_texts, targets_texts)\n",
    "            \n",
    "            if (batch_idx % 20 == 0):\n",
    "                print(\"pred:\\n\", predictions_texts[0],\"\\ntgt:\\n\", targets_texts[0])        \n",
    "        \n",
    "        batch_num = len(eval_loader)\n",
    "        running_loss /= batch_num\n",
    "        running_perplexity /= batch_num\n",
    "        running_edit_distance /= batch_num\n",
    "        \n",
    "        print('Evaluate Loss: ', running_loss, \n",
    "              'Evaluate Perplexity: ', running_perplexity,\n",
    "              'Evaluate Edit Distance:', running_edit_distance)\n",
    "\n",
    "        return running_loss, running_perplexity, running_edit_distance\n",
    "\n",
    "def train_model(model, epochs, train_loader, eval_loader, criterion, optimizer, scheduler=None, checkpoint_filename=None):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch: %d\" % (epoch))\n",
    "        \n",
    "        train_loss, train_perplexity = train_epoch(model, train_loader, criterion, optimizer)\n",
    "        eval_loss, eval_perplexity, eval_ed = evaluate_epoch(model, eval_loader, criterion)\n",
    "        \n",
    "        if scheduler:\n",
    "            if type(scheduler) is optim.lr_scheduler.StepLR:\n",
    "                scheduler.step()\n",
    "            elif type(scheduler) is optim.lr_scheduler.ReduceLROnPlateau:\n",
    "                scheduler.step(eval_loss)\n",
    "            else:\n",
    "                raise valueError(\"No such scheduler\")\n",
    "        \n",
    "        if checkpoint_filename:\n",
    "            checkpoint = {\n",
    "                \"model_state_dict\" : model.state_dict(),\n",
    "                \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\" : scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_filename)\n",
    "            print('model is saved to {}'.format(checkpoint_filename))\n",
    "        \n",
    "        print('=' * 20)\n",
    "\n",
    "def test_model(model, test_loader, save=False, filename=\"../pred/pred.csv\"):\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        # no target in test dataset/data loader\n",
    "        for batch_idx, sample in enumerate(tqdm(test_loader)):\n",
    "            assert sample.inputs.shape[1] == max(sample.inputs_lens)\n",
    "\n",
    "            inputs, inputs_lens = sample.inputs, sample.inputs_lens\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # generate mode\n",
    "            predictions = model(None, None, text=None, isTrain=False, batch_size=inputs.shape[0])\n",
    "\n",
    "            # TODO: use random search/greedy search here\n",
    "            predictions_texts = \\\n",
    "                dataloader.transform_index_to_letter(predictions.argmax(-1).detach().cpu().numpy(), index2letter, [letter2index['<eos>'], letter2index['<pad>']])            \n",
    "\n",
    "            all_preds.extend(predictions_texts)\n",
    "        \n",
    "\n",
    "    if save:\n",
    "        result = np.concatenate([np.arange(len(all_preds)).reshape(-1, 1),\n",
    "                                 np.array(all_preds).reshape(-1, 1)], axis=1)\n",
    "        np.savetxt(filename, result, fmt=\"%s\", delimiter=\",\", header=\"Id,Predicted\", comments=\"\")\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fjAJIjf5Y7h"
   },
   "outputs": [],
   "source": [
    "if colab:\n",
    "    root_path = \"drive/My Drive/11785/hw4_p2/\"\n",
    "else:\n",
    "    root_path = \"../\"\n",
    "\n",
    "pred_path = root_path + \"pred/\"\n",
    "data_path = root_path + \"data/\"\n",
    "checkpoint_path = root_path + \"checkpoint/\"\n",
    "\n",
    "ID = 1\n",
    "checkpoint_filename = checkpoint_path + \"checkpoint_%d.tar\" % ID\n",
    "pred_filename = pred_path + \"pred.csv\"\n",
    "\n",
    "train_path = data_path + \"train_new.npy\"\n",
    "train_transcripts_path = data_path + \"train_transcripts.npy\"\n",
    "dev_path = data_path + \"dev_new.npy\"\n",
    "dev_transcripts_path = data_path + \"dev_transcripts.npy\"\n",
    "test_path = data_path + \"test_new.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "st0EDOHH5Y7n"
   },
   "outputs": [],
   "source": [
    "if mode == 'actual':\n",
    "    train = np.load(train_path, allow_pickle=True, encoding='bytes')\n",
    "    train_transcripts = np.load(train_transcripts_path, allow_pickle=True, encoding='bytes')\n",
    "else:\n",
    "    train = np.load(dev_path, allow_pickle=True, encoding='bytes')\n",
    "    train_transcripts = np.load(dev_transcripts_path, allow_pickle=True, encoding='bytes')\n",
    "\n",
    "dev = np.load(dev_path, allow_pickle=True, encoding='bytes')\n",
    "dev_transcripts = np.load(dev_transcripts_path, allow_pickle=True, encoding='bytes')\n",
    "test = np.load(test_path, allow_pickle=True, encoding='bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pgS_jPFQ5Y7q"
   },
   "outputs": [],
   "source": [
    "LETTER_LIST = ['<pad>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', \\\n",
    "               'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']\n",
    "batch_size = 128 if mode == 'actual' else 32\n",
    "epochs = 100\n",
    "vocab_size = len(LETTER_LIST)\n",
    "hidden_dim = 256\n",
    "value_size=128\n",
    "key_size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NoT2V2Hb5Y7t"
   },
   "outputs": [],
   "source": [
    "letter2index, index2letter = dataloader.create_dictionaries(LETTER_LIST)\n",
    "\n",
    "train_transcripts_index = dataloader.transform_letter_to_index(\n",
    "                            train_transcripts, letter2index)\n",
    "dev_transcripts_index = dataloader.transform_letter_to_index(\n",
    "                            dev_transcripts, letter2index)\n",
    "\n",
    "train_dataset = dataloader.Speech2TextDataset(train, train_transcripts_index)\n",
    "dev_dataset = dataloader.Speech2TextDataset(dev, dev_transcripts_index)\n",
    "test_dataset = dataloader.Speech2TextDataset(test, text=None, isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8OlPl6RS5Y7w"
   },
   "outputs": [],
   "source": [
    "# TODO: may need to sort all data\n",
    "train_loader = DataLoader(\n",
    "                train_dataset,              # The dataset\n",
    "                batch_size=batch_size,      # Batch size\n",
    "                shuffle=True,               # Shuffles the dataset at every epoch\n",
    "                pin_memory=True,            # Copy data to CUDA pinned memory\n",
    "                num_workers=num_workers,    # Number of worker processes for loading data.\n",
    "                collate_fn = dataloader.collate\n",
    "               )\n",
    "\n",
    "dev_loader = DataLoader(\n",
    "                dev_dataset,              # The dataset\n",
    "                batch_size=batch_size,      # Batch size\n",
    "                shuffle=False,               # Shuffles the dataset at every epoch\n",
    "                pin_memory=True,            # Copy data to CUDA pinned memory\n",
    "                num_workers=num_workers,    # Number of worker processes for loading data.\n",
    "                collate_fn = dataloader.collate\n",
    "               )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "                test_dataset,              # The dataset\n",
    "                batch_size=batch_size,      # Batch size\n",
    "                shuffle=False,               # Shuffles the dataset at every epoch\n",
    "                pin_memory=True,            # Copy data to CUDA pinned memory\n",
    "                num_workers=num_workers,    # Number of worker processes for loading data.\n",
    "                collate_fn = dataloader.collate_test\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RtZFUZ05Y72"
   },
   "outputs": [],
   "source": [
    "# language model\n",
    "model = models.Decoder(vocab_size, hidden_dim, value_size, key_size, isAttended=False, isLM=True)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = optim.Adam(model.to(device).parameters(), lr=0.01)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwQJD1l55Y75",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(model, epochs, train_loader, dev_loader, \n",
    "            criterion, optimizer, scheduler=None, checkpoint_filename=None)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZOKUAmD5Y78"
   },
   "outputs": [],
   "source": [
    "test_predicts = test_model(model, test_loader, save=True, filename=pred_filename)\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvsywPz45Y8A"
   },
   "source": [
    "## DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeMJ072c5Y8B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
